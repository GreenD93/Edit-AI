{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe982961-7799-4d20-b5ab-dc4332d6decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "key = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5228f4a-e8ef-4779-939a-fb269fba8129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 질문: 대한민국의 수도는 어디인가요?\n",
      "## 답변\n",
      "content='대한민국의 수도는 서울입니다.' response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 24, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-8f483189-dda7-45e4-93a3-b161e5058485-0' usage_metadata={'input_tokens': 24, 'output_tokens': 15, 'total_tokens': 39}\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # 최대 토큰수\n",
    "    model_name=\"gpt-3.5-turbo\",  # 모델명\n",
    ")\n",
    "\n",
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "answer = llm.invoke(question)\n",
    "\n",
    "# 질의\n",
    "print(f\"## 질문: {question}\")\n",
    "print(f\"## 답변\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b1479f-616a-4061-95d3-9240f9fdf48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"\\nEU, 세계 최초 'AI 규제법' 통과…위반시 매출 7%까지 과징금\\n\\n\\n\\n\\n\\n\\n유럽연합(EU)이 세계 최초로 인공지능(AI)를 규제하는 법을 통과시켰다. 게티이미지뱅크 제공   유럽연합(EU)이 세계 최초로 인공지능(AI)을 규제하는 법을 통과시켰다.    13일(현지시간) 유럽의회는 프랑스 스트라스부르에서 열린 본회의에서 'AI 법(AI Act)' 최종안을 찬성 523표, 반대 46표로 가결했다. 기권은 49표였다. EU는 이 법안이 빠른 속도로 발전하는 기술의 위험으로부터 시민을 보호하는 동시에 혁신을 촉진할 것이라고 보고 있다.    법안에 따르면 EU는 AI 활용 분야를 총 4단계의 위험 등급으로 나눠 차등 규제한다. 고위험 등급으로 분류되는 의료, 교육을 비롯한 공공 서비스나 선거, 핵심 인프라, 자율주행 등에서는 AI 기술 사용 시 사람이 반드시 감독하도록 하고 위험관리시스템을 구축해야 한다.   또 광범위한 사이버 공격, ‘유해한 선입견’ 전파 등 EU가 시스템적 위험이라고 규정한 사고 발생을 방지하기 위한 조치를 해야 한다. 별도의 정보 공개·고지 의무도 부과된다.   사람과 유사한 수준 또는 그 이상의 지능을 갖춘 AI인 '범용 AI'를 개발하는 기업에 대해서는 기술 개발 과정에서의 ‘투명성 의무’를 부여하기로 했다. 이 조항은 2021년 발의된 초안에는 없었지만 이듬해 챗GPT 등 생성형 AI가 보편화되면서 AI 오남용에 대한 우려가 확산돼 입법 과정에서 추가됐다.   일부 AI 기술 활용은 원천 금지된다. 이른바 개인의 특성·행동과 관련된 데이터로 개별 점수를 매기는 관행인 ‘사회적 점수 평가’(소셜 스코어링)가 대표적이다. AI를 활용한 실시간 원격 생체인식 식별 시스템 사용도 중대 범죄 용의자 수색을 비롯해 예외적인 경우를 제외하곤 사실상 금지된다. 이 밖에 딥페이크 영상이나 이미지는 AI로 만든 조작 콘텐츠라는 점을 표기하도록 했다.   이번에 통과된 법안은 EU 27개국 장관들이 내달 최종 승인하고 오는 6월 EU 관보에 게재된 뒤 발효된다. 일부 금지 조항은 발효 뒤 6개월부터 적용되며 이후 단계적으로 도입돼 2026년 이후 전면 시행된다. 법을 위반할 경우 경중에 따라 전 세계 매출의 1.5%에서 최대 7%에 해당하는 과징금이 부과될 수 있다.\\n\\n\", metadata={'source': 'https://n.news.naver.com/mnews/article/584/0000026285'})]\n",
      "######################################################################\n",
      "## chunking ##\n",
      "## page_content=\"EU, 세계 최초 'AI 규제법' 통과…위반시 매출 7%까지 과징금\" metadata={'source': 'https://n.news.naver.com/mnews/article/584/0000026285'}\n",
      "\n",
      "## page_content=\"유럽연합(EU)이 세계 최초로 인공지능(AI)를 규제하는 법을 통과시켰다. 게티이미지뱅크 제공   유럽연합(EU)이 세계 최초로 인공지능(AI)을 규제하는 법을 통과시켰다.    13일(현지시간) 유럽의회는 프랑스 스트라스부르에서 열린 본회의에서 'AI 법(AI Act)' 최종안을 찬성 523표, 반대 46표로 가결했다. 기권은 49표였다. EU는 이 법안이 빠른 속도로 발전하는 기술의 위험으로부터 시민을 보호하는 동시에 혁신을 촉진할 것이라고 보고 있다.    법안에 따르면 EU는 AI 활용 분야를 총 4단계의 위험 등급으로 나눠 차등 규제한다. 고위험 등급으로 분류되는 의료, 교육을 비롯한 공공 서비스나 선거, 핵심 인프라, 자율주행 등에서는 AI 기술 사용 시 사람이 반드시 감독하도록 하고 위험관리시스템을 구축해야 한다.   또 광범위한 사이버 공격, ‘유해한 선입견’ 전파 등 EU가 시스템적 위험이라고 규정한 사고 발생을 방지하기 위한 조치를 해야 한다. 별도의 정보\" metadata={'source': 'https://n.news.naver.com/mnews/article/584/0000026285'}\n",
      "\n",
      "## page_content=\"시스템적 위험이라고 규정한 사고 발생을 방지하기 위한 조치를 해야 한다. 별도의 정보 공개·고지 의무도 부과된다.   사람과 유사한 수준 또는 그 이상의 지능을 갖춘 AI인 '범용 AI'를 개발하는 기업에 대해서는 기술 개발 과정에서의 ‘투명성 의무’를 부여하기로 했다. 이 조항은 2021년 발의된 초안에는 없었지만 이듬해 챗GPT 등 생성형 AI가 보편화되면서 AI 오남용에 대한 우려가 확산돼 입법 과정에서 추가됐다.   일부 AI 기술 활용은 원천 금지된다. 이른바 개인의 특성·행동과 관련된 데이터로 개별 점수를 매기는 관행인 ‘사회적 점수 평가’(소셜 스코어링)가 대표적이다. AI를 활용한 실시간 원격 생체인식 식별 시스템 사용도 중대 범죄 용의자 수색을 비롯해 예외적인 경우를 제외하곤 사실상 금지된다. 이 밖에 딥페이크 영상이나 이미지는 AI로 만든 조작 콘텐츠라는 점을 표기하도록 했다.   이번에 통과된 법안은 EU 27개국 장관들이 내달 최종 승인하고 오는 6월 EU\" metadata={'source': 'https://n.news.naver.com/mnews/article/584/0000026285'}\n",
      "\n",
      "## page_content='이번에 통과된 법안은 EU 27개국 장관들이 내달 최종 승인하고 오는 6월 EU 관보에 게재된 뒤 발효된다. 일부 금지 조항은 발효 뒤 6개월부터 적용되며 이후 단계적으로 도입돼 2026년 이후 전면 시행된다. 법을 위반할 경우 경중에 따라 전 세계 매출의 1.5%에서 최대 7%에 해당하는 과징금이 부과될 수 있다.' metadata={'source': 'https://n.news.naver.com/mnews/article/584/0000026285'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "url = \"https://n.news.naver.com/mnews/article/584/0000026285\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)\n",
    "\n",
    "print('#'*70)\n",
    "print('## chunking ##')\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "for tmp in splits:\n",
    "    print(f'## {tmp}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72b6149-0ee8-4592-b457-2c2f0480c48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchainhub in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (0.1.17)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from langchainhub) (2.32.0.20240523)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from requests<3,>=2->langchainhub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from requests<3,>=2->langchainhub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/databiz/miniconda3/envs/llm/lib/python3.8/site-packages (from requests<3,>=2->langchainhub) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f5671c-8a31-4d9a-ae9d-5e0073c3a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 4: 검색(Search)\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 5: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 6: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 7: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceaa3b9b-6753-4a0b-9ea6-2dc639d88919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://n.news.naver.com/mnews/article/584/0000026285\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "ai 규제법에 대해 알려줘\n",
      "\n",
      "[AI]\n",
      "유럽연합(EU)은 세계 최초로 인공지능(AI)를 규제하는 법을 통과시켰다. AI 규제법은 AI 활용 분야를 4단계의 위험 등급으로 나누어 차등 규제하며, 고위험 등급에서는 사람 감독과 위험관리시스템 구축이 요구된다. 법을 위반할 경우 최대 7%까지의 과징금이 부과될 수 있다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"ai 규제법에 대해 알려줘\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7110cde-1a78-47d4-b669-71e81ab5a9cb",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf30ec59-c740-42ba-aaf5-18c82bb87400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 텍스트로부터 FAISS 벡터 저장소를 생성합니다.\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"테디는 랭체인 주식회사에서 근무를 하였습니다.\",\n",
    "        \"셜리는 테디와 같은 회사에서 근무하였습니다.\",\n",
    "        \"테디의 직업은 개발자입니다.\",\n",
    "        \"셜리의 직업은 디자이너입니다.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "# 벡터 저장소를 검색기로 사용합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "# 템플릿을 정의합니다.\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# 템플릿으로부터 채팅 프롬프트를 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# ChatOpenAI 모델을 초기화합니다.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 검색 체인을 구성합니다.\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cc65fe-2b3a-41b8-bda2-50282f5288f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5eb3d8c-eb5e-4808-bf3b-e40d2cb6677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테디의 직업은 개발자입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.\n",
    "retrieval_chain.invoke(\"테디의 직업은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68e56da-fcac-4794-8289-282d7ecb5340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'셜리의 직업은 디자이너입니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.\n",
    "retrieval_chain.invoke(\"셜리의 직업은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25a7ad-1711-4a7b-b9e5-2b5452c4cb8a",
   "metadata": {},
   "source": [
    "## WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4effca-1802-4b40-a2c2-9fd9e4e640c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Missing_Entities\": \"\",\n",
      "        \"Denser_Summary\": \"이 기사는 데이터사이언스, 머신러닝, 인공지능에 대한 개념을 설명하고 있습니다. 박정현 서울대 EPM 연구원이 작성한 이 글에서는 이 세 분야가 우리 생활에 어떻게 적용되고 있는지, 그리고 이 분야들을 공부하고자 하는 사람들이 어떻게 접근해야 하는지에 대한 정보를 제공합니다. 또한, 이 분야들의 정의와 차이점, 그리고 각 분야에서 사용되는 기술들에 대한 설명도 포함되어 있습니다. 이러한 내용을 통해 독자들은 데이터사이언스, 머신러닝, 인공지능에 대한 기본적인 이해를 할 수 있게 됩니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터사이언스 대학원; 인공지능 국가전략; 지도학습; 비지도 학습\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원은 데이터사이언스, 머신러닝, 인공지능의 개념과 실생활 적용 예를 설명하며, 이 분야들의 공부 방향을 제시합니다. 데이터사이언스 대학원 설립과 인공지능 국가전략 발표 등 교육 및 정책 변화를 언급하고, 머신러닝의 지도학습과 비지도 학습을 포함한 기술적 접근 방식을 설명합니다. 이 글은 이 분야에 관심 있는 이들에게 기초적인 이해와 함께 학습 및 진로 탐색에 도움을 주는 정보를 담고 있습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"알파고; GPT-3; 튜링테스트; k-means\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용 예제(알파고, GPT-3)를 소개하고, 데이터사이언스 대학원과 인공지능 국가전략의 중요성을 강조합니다. 머신러닝의 지도학습, 비지도 학습 방식과 튜링테스트를 통한 인공지능의 이해, k-means 같은 알고리즘 사용을 설명하며, 이 분야의 학습자와 전문가를 위한 지침을 제공합니다. 이 글은 기술적 배경과 함께 정책 및 교육적 변화를 통해 독자들에게 심층적인 인사이트를 제공합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터 피처 엔지니어링; 알고리즘; 성능평가\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 기본 개념과 알파고, GPT-3 같은 적용 사례를 설명합니다. 데이터사이언스 대학원, 인공지능 국가전략의 발표, 지도학습, 비지도 학습, 튜링테스트, k-means 알고리즘 등을 언급하며, 데이터 피처 엔지니어링, 다양한 알고리즘 사용, 성능평가 방법에 대한 정보를 제공합니다. 이 글은 이 분야의 기술적, 교육적, 정책적 측면을 아우르며 독자들에게 깊은 이해를 돕습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"정형 및 비정형 데이터; 이상값 검출; 강화학습\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의, 알파고와 GPT-3의 사례를 통해 이 분야의 적용을 설명합니다. 데이터사이언스 대학원 설립, 인공지능 국가전략, 지도학습, 비지도 학습, 튜링테스트, k-means, 데이터 피처 엔지니어링, 알고리즘, 성능평가 방법을 포함해 정형 및 비정형 데이터의 활용, 이상값 검출, 강화학습 등의 개념을 소개하며, 이 분야의 전문가가 되기 위한 방향을 제시합니다. 이 글은 기술적 깊이와 함께 교육 및 정책적 관점을 제공합니다.\"\n",
      "    }\n",
      "]\n",
      "```박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의, 알파고와 GPT-3의 사례를 통해 이 분야의 적용을 설명합니다. 데이터사이언스 대학원 설립, 인공지능 국가전략, 지도학습, 비지도 학습, 튜링테스트, k-means, 데이터 피처 엔지니어링, 알고리즘, 성능평가 방법을 포함해 정형 및 비정형 데이터의 활용, 이상값 검출, 강화학습 등의 개념을 소개하며, 이 분야의 전문가가 되기 위한 방향을 제시합니다. 이 글은 기술적 깊이와 함께 교육 및 정책적 관점을 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\"https://www.aitimes.com/news/articleView.html?idxno=131777\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"teddynote/chain-of-density-korean\")\n",
    "\n",
    "# Create the chain, including\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    )\n",
    "    | JsonOutputParser()\n",
    "    | (lambda x: x[-1][\"Denser_Summary\"])\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"ARTICLE\": content})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab774729-8e94-4a2a-8a02-87d625082821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ARTICLE'], template='Article: {ARTICLE}\\nYou will generate increasingly concise, entity-dense summaries of the above article. \\n\\nRepeat the following 2 steps 5 times. \\n\\nStep 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \\nStep 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \\n\\nA missing entity is:\\n- relevant to the main story, \\n- specific yet concise (100 words or fewer), \\n- novel (not in the previous summary), \\n- faithful (present in the article), \\n- anywhere (can be located anywhere in the article).\\n\\nGuidelines:\\n\\n- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\\n- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \\n- Missing entities can appear anywhere in the new summary.\\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \\n\\nRemember, use the exact same number of words for each summary.\\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\\nUse only KOREAN language to reply.'))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ed5e2e-df3b-4eba-a989-434c3e124bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Article: {ARTICLE}\n",
      "You will generate increasingly concise, entity-dense summaries of the above article. \n",
      "\n",
      "Repeat the following 2 steps 5 times. \n",
      "\n",
      "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
      "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
      "\n",
      "A missing entity is:\n",
      "- relevant to the main story, \n",
      "- specific yet concise (100 words or fewer), \n",
      "- novel (not in the previous summary), \n",
      "- faithful (present in the article), \n",
      "- anywhere (can be located anywhere in the article).\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
      "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
      "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
      "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
      "- Missing entities can appear anywhere in the new summary.\n",
      "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
      "\n",
      "Remember, use the exact same number of words for each summary.\n",
      "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
      "Use only KOREAN language to reply.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "'Article: {ARTICLE}\\nYou will generate increasingly concise, entity-dense summaries of the above article. \\n\\nRepeat the following 2 steps 5 times. \\n\\nStep 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \\nStep 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \\n\\nA missing entity is:\\n- relevant to the main story, \\n- specific yet concise (100 words or fewer), \\n- novel (not in the previous summary), \\n- faithful (present in the article), \\n- anywhere (can be located anywhere in the article).\\n\\nGuidelines:\\n\\n- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\\n- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\\n- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\\n- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \\n- Missing entities can appear anywhere in the new summary.\\n- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \\n\\nRemember, use the exact same number of words for each summary.\\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\\nUse only KOREAN language to reply.'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
